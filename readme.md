# Datasets
## Synthetic Dataset:  
In the 'dataset/synthetic_data' folder we provide a randomly constructed synthetic dataset. 
The dataset is constructed as follows:
First, we generate two synthetic user groups, representing normal and abnormal users, respectively. 
The relationship between normal users is generated by the Barabási–Albert graph model, 
while the relationship between abnormal users is generated by the random graph algorithm. 
We use Gaussian distribution to randomly generate user features, the mean and standard deviation 
of these two types of users are randomly sampled from [-5, 5] and [1, 10], respectively. 
Second, we randomly connect users between two groups to simulate the relationship between two 
different types of users. Third, we sample 20% of abnormal (normal) users into the normal (abnormal) 
graph so that a graph structure consists of two different types of users. 
We sample 100 users per graph, and generate 50 normal graphs and 50 abnormal graphs. 
The label of each graph is determined by the major labels of the users in the graph. 
Finally, we construct the connections between two graphs if they have more than one common user.  

The datafile 'edge_index_list.txt' ('group_edge_index_list.txt') contains the relationship between users (groups).

The datafile 'user_label_list.txt' ('group_label_list.txt') contains the label of each user (group).

The datafile 'trainNode.txt' ('trainGroup.txt') and 'testNode.txt' (testGroup.txt) contain the indexs of nodes (groups)
randomly sampled to train and test the model, respectively.

## PHEME Dataset:  
The real-world datasets used in the experiments were based on the publicly available PHEME datasets released by Arkaitz et al. (2016):

Zubiaga Arkaitz, Maria Liakata, and Rob Procter. "Learning reporting dynamics during breaking news for rumour detection in social media." arXiv preprint arXiv:1610.07363 (2016).

In the 'dataset/PHEME_data' folder we provide the pre-processed data files used for our experiments. The raw data can be downloaded at [figshare](https://figshare.com/articles/PHEME_dataset_of_rumours_and_non-rumours/4010619).

The datafile event+'.txt' (such as 'charliehebdo.txt') is in a tab-separated column format, where each row corresponds to a tweet user. Consecutive columns correspond to the following pieces of information:  
1: label-of-topic -- the label of the topic related to the event;  
2: root-id -- an unique identifier describing the topic (tweetid of the source tweet);  
3: index-of-parent-user -- an index number of the parent user for the current user;  
4: index-of-the-current-user -- an index number of the current user;  
5: list-of-features -- the rest of the line contains space separated features of each user extracted from the raw data, the features consists of "profile_use_background_image", 
"default_profile_image", "verified", "whether_has_profile_location", "followers_count", "listed_count", "statuses_count", "whether_has_description", 
"friends_count", "favourites_count", "created_time";  
6: user-id -- an unique identifier describing the user.

The datafile event+occur_times+'_rumor_user.txt' (such as 'ferguson2_rumor_user.txt') contains the users' ID that appear in more than 'occur_times' rumor topics in the 'event'.

The datafile event_name+'_group.json' (such as 'sydneysiege_group.json') records the topic indexs where the user exists. 

## S3DIS Dataset:
The point cloud segmentation datasets used in the experiments were based on the publicly available S3DIS datasets released by Armeni
et al. (2016):

Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer and Silvio Savarese. 3d semantic parsing of
large-scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1534–1543, 2016.

In the 'dataset' folder, we provide 'gen_S3DIS.py' to generate pre-processed data used for our experiments.

Run
```
python3 ./dataset/gen_S3DIS.py --obj 2 --data 'train'
```
to generate processed data for different segmentation object.
There are two arguments are required to be specified:
1. Choose the 'obj' from 0~12 to reproduce the experimental results on different object dataset.
2. Choose the 'data' from ['train', 'test'] to generate the test set or the training set.


# Dependencies 
python==3.6
numpy==1.19.5
torch==1.7.1
scikit-learn==0.24.0  
torch_scatter==2.0.5
torch_sparse==0.6.8
torch_cluster==1.5.8  
torch_geometric==1.6.3  
tqdm==4.55.1

Make sure that cuda/bin, cuda/include and cuda/lib64 are in your $PATH, $CPATH and $LD_LIBRARY_PATH respectively before the installation, e.g.:
```
$ echo $PATH
>>> /usr/local/cuda/bin:...

$ echo $CPATH
>>> /usr/local/cuda/include:...
```
and
```
$ echo $LD_LIBRARY_PATH
>>> /usr/local/cuda/lib64
```
on Linux or
```
$ echo $DYLD_LIBRARY_PATH
>>> /usr/local/cuda/lib
```
on macOS. 

# Reproduce the experimental results
Run script 
```
$ sh main.sh
```
and choose "synthetic_model/main.py" to reproduce the experimental results for different models on synthetic dataset. 
At least two arguments need to be specified:  
1: Choose the 'model_name' from ['GMGCN', 'ATTGCN'].  
2: Choose the 'hier' from ['W', 'WO'] to determine whether the model has a hierarchical graph structure.   
E.g., run
```
python3 ./synthetic_model/main.py --model_name 'GMGCN' --hier 'W'
```
will reproduce the experimental results of of GMGCN model on synthetic dataset with a hierarchical graph structure.
  
Meanwhile, run
```
python3 ./PHEME_model/main.py --data_name 'germanwings-crash' --occur_times 4
```
will reproduce the experimental results of GMGCN on PHEME dataset. There are also at least two arguments are required to be specified:  
1. Choose the 'data_name' from ['charliehebdo', 'ferguson', 'germanwings-crash', 'ottawashooting', 'sydneysiege'] to reproduce the experimental results for different event.  
2. Choose the 'occur_times' that determines the user labels from [2,3,4].

Run
```
python3 ./S3DIS_model/main.py --obj 2  --multi_gpus  --testdata 'train'
```  
will reproduce the experimental results of GMGCN on S3DIS dataset. 
There are also at least three arguments are required to be specified:
1. Choose the 'obj' from 0~12 to reproduce the experimental results on different object dataset.  
2. Choose the 'multi_gpus' that determines whether to use multiple GPUs for parallel computing.
3. Choose the 'testdata' from ['train', 'test'] to determine whether to test the model on the test set or the training set.


